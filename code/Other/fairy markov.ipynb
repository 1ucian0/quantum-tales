{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-order Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from re import sub, split\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('words')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_list = nltk.corpus.words.words()\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "from numpy import random, array\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for file in glob('corpus/*'):\n",
    "    text += open(file, 'r', encoding='utf8').read() + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    text = []\n",
    "    \n",
    "    words = nltk.word_tokenize(string)\n",
    "    text.extend([word for word in words if word.isalpha()])\n",
    "\n",
    "    return ' '.join(list(map(str.lower, text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scifi_text = ''\n",
    "for file in glob('/Users/splch/Desktop/opposite/*'):\n",
    "    scifi_text += open(file, 'r', encoding='utf8').read() + '\\n'\n",
    "scifi_text = clean(scifi_text)\n",
    "scifi_stems = set(map(stemmer.stem, map(str.lower, set(scifi_text.split(' ')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196671/196671 [05:40<00:00, 578.32it/s]\n"
     ]
    }
   ],
   "source": [
    "words = {}\n",
    "for sent in tqdm(nltk.sent_tokenize(text)):\n",
    "    s = sub(r'\\s+', ' ', sent)\n",
    "    s = nltk.pos_tag(nltk.word_tokenize(s))\n",
    "    for word, pos in s:\n",
    "        stem = stemmer.stem(word).lower()\n",
    "        if word not in stopwords and word.isalpha() and stem not in scifi_stems:\n",
    "            if stem not in words:\n",
    "                words[stem] = {(word.lower(), pos): 1}\n",
    "            else:\n",
    "                words[stem][(word.lower(), pos)] = words[stem].get((word.lower(), pos), 0) + 1\n",
    "words_dict = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = {}\n",
    "for stem_key, stem_value in words_dict.items():\n",
    "    count = 0\n",
    "    big = ('', 0)\n",
    "    for word_key, word_value in stem_value.items():\n",
    "        if word_value > count:\n",
    "            big = word_key\n",
    "            count = word_value\n",
    "\n",
    "    if count > final_list.get(big[0], ('', 0))[1]:\n",
    "        final_list[big[0]] = (big[1], count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fairy_tale_words.txt', 'w') as f:\n",
    "    for item in sorted(final_list.items(), key=lambda x: -x[1][1]):\n",
    "        if item[1][1] > 10:\n",
    "            f.write(f'{item[0]}: {item[1][0]}, {item[1][1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# markov.py\n",
    "\n",
    "from itertools import tee\n",
    "\n",
    "\n",
    "def update(d, keys, value):\n",
    "    for key in keys:\n",
    "        if key in d:\n",
    "            d = d[key]\n",
    "        else:\n",
    "            newd = dict()\n",
    "            d[key] = newd\n",
    "            d = newd\n",
    "    d[value] = d.get(value, 0) + 1\n",
    "\n",
    "\n",
    "def marginalizeF(fname, window):\n",
    "    with open(fname, 'r', encoding='utf8') as f:\n",
    "        return marginalize(f.read(), window)\n",
    "\n",
    "\n",
    "def marginalize(text, window):\n",
    "    d = dict()\n",
    "    for w in slide(text.split(' '), window):\n",
    "        features = w[:-1]\n",
    "        target = w[-1]\n",
    "        update(d, features, target)\n",
    "    return d\n",
    "\n",
    "\n",
    "def slide(iterable, size):\n",
    "    iters = tee(iterable, size)\n",
    "    for i in range(1, size):\n",
    "        for each in iters[i:]:\n",
    "            next(each, None)\n",
    "    return zip(*iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 4\n",
    "# text = open('tales.txt', 'r', encoding='utf8').read()\n",
    "\n",
    "d = marginalize(text, int(window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: once upon a time an old woman putting out her lanterns you will light the candle for you and then he said o heavens why have not my mother \n",
      "\n",
      "2: once upon a time there was a fierce beast is in my iwanich thanked the old man thought in his mind while people were asleep he went out cut \n",
      "\n",
      "3: once upon a time there was a fire in my veins doth flow yet i laugh and sing the nurse asked the boatman can you take the fellow out \n",
      "\n",
      "4: once upon a time there lived an emperor whose name was souci and he had not gone far before he met a squirrel who bowed and hid it in \n",
      "\n",
      "5: once upon a time there was a big palmyra tree lying on the road the snow was so wet it certainly must have come from and how you came \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_words = 25\n",
    "n_sents = 5\n",
    "\n",
    "for i in range(n_sents):\n",
    "    chain = ['once', 'upon', 'a', 'time']\n",
    "    for j in range(n_words):\n",
    "        window_d = chain[1 - window:]\n",
    "        d_t = d[window_d[0]]\n",
    "        for key in range(1, window - 1):\n",
    "            d_t = d_t.get(window_d[key], {})\n",
    "        vals = list(d_t.values())\n",
    "        word = random.choice(\n",
    "            list(d_t.keys()),\n",
    "            p=array(vals)/sum(vals)\n",
    "        )\n",
    "        chain.append(word)\n",
    "    print(str(i+1)+':', ' '.join(chain), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = request.urlopen('https://docs.google.com/document/export?format=txt&id=1nlGzXv09roHMtTjlJQhJ6ZnwWMDHeGKi_Xnk8mygjEw').read().decode('utf-8').replace('\\r\\n', '\\n').replace('\\ufeff', '')\n",
    "doc_words = clean(doc).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chain = doc_words[:window - 1]\n",
    "outliers = {}\n",
    "\n",
    "for i, word in enumerate(doc_words[window - 1:]):\n",
    "    d_t = d.get(doc_words[i], {})\n",
    "    key = 1\n",
    "    for key in range(1, window - 1):\n",
    "        d_t = d_t.get(doc_words[i + key], {})\n",
    "    if i + key + 1 < len(doc_words):\n",
    "        if doc_words[i + key + 1] not in d_t:\n",
    "            outliers[doc_words[i + key + 1]] = outliers.get(doc_words[i + key + 1], 0) + 1\n",
    "    chain.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_words = set()\n",
    "for key, value in outliers.items():\n",
    "    if value / doc_words.count(key) == 1 and key not in stopwords:\n",
    "        outlier_words.add(key)\n",
    "# print(outlier_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairy_stems = set(map(stemmer.stem, set(text.split(' '))))\n",
    "\n",
    "def check_fairy_word(word):\n",
    "    return stemmer.stem(word) in fairy_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_words = {}\n",
    "for word in outlier_words:\n",
    "    stem = stemmer.stem(word)\n",
    "    if stem not in fairy_stems and word in word_list:\n",
    "        if stem in check_words:\n",
    "            check_words[stem].append(word)\n",
    "        else:\n",
    "            check_words[stem] = [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'protagonist': ['protagonist'],\n 'electron': ['electronic'],\n 'modulus': ['modulus'],\n 'cipher': ['cipher'],\n 'hanna': ['hanna'],\n 'unitari': ['unitary'],\n 'encod': ['encode'],\n 'comput': ['computation', 'computer'],\n 'rand': ['rand'],\n 'coincident': ['coincidentally', 'coincidental'],\n 'amplifi': ['amplify'],\n 'rubbl': ['rubble'],\n 'travi': ['travis'],\n 'repositori': ['repository'],\n 'copyright': ['copyright'],\n 'uncloth': ['unclothed'],\n 'ocher': ['ocher'],\n 'paperback': ['paperback'],\n 'rematch': ['rematch'],\n 'millennia': ['millennia'],\n 'browser': ['browser'],\n 'introspect': ['introspective'],\n 'skid': ['skidding'],\n 'algorithm': ['algorithm'],\n 'feedback': ['feedback'],\n 'focus': ['focus'],\n 'encrypt': ['encrypt'],\n 'shor': ['shor'],\n 'unscrambl': ['unscramble'],\n 'edit': ['edition'],\n 'slowpok': ['slowpoke'],\n 'unord': ['unordered'],\n 'fictiti': ['fictitiously'],\n 'photon': ['photon'],\n 'typic': ['typical'],\n 'notebook': ['notebook'],\n 'specif': ['specifically'],\n 'bibliographi': ['bibliography']}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_fairy_word(\"breakfast\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}