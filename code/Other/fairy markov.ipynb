{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-order Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from re import sub, split\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('words')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "import re\n",
    "\n",
    "from numpy import random, array\n",
    "\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for file in glob('corpus/*'):\n",
    "    if 'preface' in file.lower():\n",
    "        print(file)\n",
    "    text += open(file, 'r', encoding='utf8').read() + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    text = []\n",
    "    \n",
    "    words = nltk.word_tokenize(string)\n",
    "    text.extend([word for word in words if word.isalpha()])\n",
    "\n",
    "    return ' '.join(list(map(str.lower, text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# markov.py\n",
    "\n",
    "from itertools import tee\n",
    "\n",
    "\n",
    "def update(d, keys, value):\n",
    "    for key in keys:\n",
    "        if key in d:\n",
    "            d = d[key]\n",
    "        else:\n",
    "            newd = dict()\n",
    "            d[key] = newd\n",
    "            d = newd\n",
    "    d[value] = d.get(value, 0) + 1\n",
    "\n",
    "\n",
    "def marginalizeF(fname, window):\n",
    "    with open(fname, 'r', encoding='utf8') as f:\n",
    "        return marginalize(f.read(), window)\n",
    "\n",
    "\n",
    "def marginalize(text, window):\n",
    "    d = dict()\n",
    "    for w in slide(text.split(' '), window):\n",
    "        features = w[:-1]\n",
    "        target = w[-1]\n",
    "        update(d, features, target)\n",
    "    return d\n",
    "\n",
    "\n",
    "def slide(iterable, size):\n",
    "    iters = tee(iterable, size)\n",
    "    for i in range(1, size):\n",
    "        for each in iters[i:]:\n",
    "            next(each, None)\n",
    "    return zip(*iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 4\n",
    "# text = open('tales.txt', 'r', encoding='utf8').read()\n",
    "\n",
    "d = marginalize(text, int(window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: once upon a time there was a great feast what do you want you must answer cherries and when he found himself in a room bare except for one \n\n2: once upon a time in husband said she it is zizi exclaimed desire who entered at this moment the came up to the woodman and when i call you \n\n3: once upon a time a soldier who for many years at last i begged her not to tell thy wife the truth for seven years and now has blinded \n\n4: once upon a time the king gave them presents of silver and glittered and sparkled with all colours so beautifully that the children stood still and waited leaning on \n\n5: once upon a time three waggons of gold and precious stones there were also several small chickens running about and hunger he fell asleep the sun had grown low \n\n"
     ]
    }
   ],
   "source": [
    "n_words = 25\n",
    "n_sents = 5\n",
    "\n",
    "for i in range(n_sents):\n",
    "    chain = ['once', 'upon', 'a', 'time']\n",
    "    for j in range(n_words):\n",
    "        window_d = chain[1 - window:]\n",
    "        d_t = d[window_d[0]]\n",
    "        for key in range(1, window - 1):\n",
    "            d_t = d_t.get(window_d[key], {})\n",
    "        vals = list(d_t.values())\n",
    "        word = random.choice(\n",
    "            list(d_t.keys()),\n",
    "            p=array(vals)/sum(vals)\n",
    "        )\n",
    "        chain.append(word)\n",
    "    print(str(i+1)+':', ' '.join(chain), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = request.urlopen('https://docs.google.com/document/export?format=txt&id=1nlGzXv09roHMtTjlJQhJ6ZnwWMDHeGKi_Xnk8mygjEw').read().decode('utf-8').replace('\\r\\n', '\\n').replace('\\ufeff', '')\n",
    "doc_words = clean(doc).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chain = doc_words[:window - 1]\n",
    "outliers = {}\n",
    "\n",
    "for i, word in enumerate(doc_words[window - 1:]):\n",
    "    d_t = d.get(doc_words[i], {})\n",
    "    key = 1\n",
    "    for key in range(1, window - 1):\n",
    "        d_t = d_t.get(doc_words[i + key], {})\n",
    "    if i + key + 1 < len(doc_words):\n",
    "        if doc_words[i + key + 1] not in d_t:\n",
    "            outliers[doc_words[i + key + 1]] = outliers.get(doc_words[i + key + 1], 0) + 1\n",
    "    chain.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_words = set()\n",
    "for key, value in outliers.items():\n",
    "    if value / doc_words.count(key) == 1 and key not in stopwords:\n",
    "        outlier_words.add(key)\n",
    "# print(outlier_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "fairy_stems = set(map(stemmer.stem, set(text.split(' '))))\n",
    "\n",
    "def check_fairy_word(word):\n",
    "    return stemmer.stem(word) in fairy_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "word_list = words.words()\n",
    "\n",
    "check_words = {}\n",
    "for word in outlier_words:\n",
    "    stem = stemmer.stem(word)\n",
    "    if stem not in fairy_stems and word in word_list:\n",
    "        if stem in check_words:\n",
    "            check_words[stem].append(word)\n",
    "        else:\n",
    "            check_words[stem] = [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'shor': ['shor'],\n",
       " 'paperback': ['paperback'],\n",
       " 'skid': ['skidding'],\n",
       " 'feedback': ['feedback'],\n",
       " 'encod': ['encode'],\n",
       " 'specif': ['specifically'],\n",
       " 'slowpok': ['slowpoke'],\n",
       " 'cipher': ['cipher'],\n",
       " 'hanna': ['hanna'],\n",
       " 'electron': ['electronic'],\n",
       " 'unscrambl': ['unscramble'],\n",
       " 'focus': ['focus'],\n",
       " 'fictiti': ['fictitiously'],\n",
       " 'unitari': ['unitary'],\n",
       " 'unord': ['unordered'],\n",
       " 'edit': ['edition'],\n",
       " 'bibliographi': ['bibliography'],\n",
       " 'photon': ['photon'],\n",
       " 'notebook': ['notebook'],\n",
       " 'coincident': ['coincidental', 'coincidentally'],\n",
       " 'travi': ['travis'],\n",
       " 'encrypt': ['encrypt'],\n",
       " 'uncloth': ['unclothed'],\n",
       " 'amplifi': ['amplify'],\n",
       " 'rubbl': ['rubble'],\n",
       " 'copyright': ['copyright'],\n",
       " 'rematch': ['rematch'],\n",
       " 'browser': ['browser'],\n",
       " 'rand': ['rand'],\n",
       " 'protagonist': ['protagonist'],\n",
       " 'ocher': ['ocher'],\n",
       " 'comput': ['computer', 'computation'],\n",
       " 'typic': ['typical'],\n",
       " 'modulus': ['modulus'],\n",
       " 'millennia': ['millennia'],\n",
       " 'repositori': ['repository'],\n",
       " 'algorithm': ['algorithm']}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "check_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "check_fairy_word(\"modest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}